{"data_mtime": 1614926896, "dep_lines": [1, 2, 5, 5, 6, 7, 12, 17, 24, 29, 39, 41, 42, 43, 45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "dep_prios": [10, 5, 10, 20, 10, 5, 5, 5, 5, 5, 5, 5, 10, 10, 5, 5, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30], "dependencies": ["torch", "torch.fx.graph", "torch.nn.quantized", "torch.nn", "torch.nn.quantized.dynamic", "torch.quantization", "torch.quantization.quantization_mappings", "torch.quantization.utils", "torch.quantization.fx.pattern_utils", "torch.quantization.fx.utils", "torch.quantization.fx.quantization_types", "abc", "operator", "warnings", "typing", "builtins", "_importlib_modulespec", "torch._C", "torch._C._VariableFunctions", "torch._C._nn", "torch._ops", "torch.fx", "torch.fx.node", "torch.nn.functional", "torch.nn.intrinsic", "torch.nn.intrinsic.modules", "torch.nn.intrinsic.modules.fused", "torch.nn.intrinsic.qat", "torch.nn.intrinsic.qat.modules", "torch.nn.intrinsic.qat.modules.conv_fused", "torch.nn.intrinsic.qat.modules.linear_relu", "torch.nn.intrinsic.quantized", "torch.nn.intrinsic.quantized.modules", "torch.nn.intrinsic.quantized.modules.linear_relu", "torch.nn.modules", "torch.nn.modules.activation", "torch.nn.modules.batchnorm", "torch.nn.modules.container", "torch.nn.modules.conv", "torch.nn.modules.dropout", "torch.nn.modules.instancenorm", "torch.nn.modules.linear", "torch.nn.modules.module", "torch.nn.modules.normalization", "torch.nn.modules.pooling", "torch.nn.modules.rnn", "torch.nn.modules.sparse", "torch.nn.qat", "torch.nn.qat.modules", "torch.nn.qat.modules.conv", "torch.nn.qat.modules.linear", "torch.nn.quantized.dynamic.modules", "torch.nn.quantized.dynamic.modules.linear", "torch.nn.quantized.modules", "torch.nn.quantized.modules.linear", "torch.quantization.fake_quantize", "torch.quantization.quantize_fx", "torch.tensor"], "hash": "ae6d47f227f01a2d8a84d7e30e8b3acabbaceecfae67737a1a5cea171f442a5b", "id": "torch.quantization.fx.quantization_patterns", "ignore_all": true, "interface_hash": "468b3ea5c5bb7c25b86e6dff427035b4e20d7464e25a92d282ac4e53026aec85", "mtime": 1614655161, "options": {"allow_redefinition": false, "allow_untyped_globals": false, "always_false": [], "always_true": [], "bazel": false, "check_untyped_defs": false, "disallow_any_decorated": false, "disallow_any_explicit": false, "disallow_any_expr": false, "disallow_any_generics": false, "disallow_any_unimported": false, "disallow_incomplete_defs": false, "disallow_subclassing_any": false, "disallow_untyped_calls": false, "disallow_untyped_decorators": false, "disallow_untyped_defs": false, "follow_imports": "silent", "follow_imports_for_stubs": false, "ignore_errors": false, "ignore_missing_imports": true, "implicit_reexport": true, "local_partial_types": false, "mypyc": false, "no_implicit_optional": false, "platform": "linux", "plugins": [], "show_none_errors": true, "strict_equality": false, "strict_optional": true, "strict_optional_whitelist": null, "warn_no_return": true, "warn_return_any": false, "warn_unreachable": false, "warn_unused_ignores": false}, "path": "/data/users/cdhernandez/pytorch/torch/quantization/fx/quantization_patterns.py", "plugin_data": null, "size": 48730, "suppressed": [], "version_id": "0.770"}